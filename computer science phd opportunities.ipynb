{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64061dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27e4f141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "findphd_url = 'https://www.findaphd.com/phds/computer-science/non-eu-students/?11M7g40&PG={}'\n",
    "#keyword_param = 'artificial intelligence'\n",
    "ind= 1\n",
    "headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.51'\n",
    "\n",
    "    }\n",
    "phd_df = pd.DataFrame(columns=['Title', 'University', 'Professor', 'Description', 'Deadline'])\n",
    "url = findphd_url.format(1) # Getting url from page 1 of result\n",
    "response = requests.get(url, headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "total_page = soup.find('div', class_='pager').get_text().split('.')[-1] if soup.find('div', class_='pager') is not None else ''\n",
    "total_page = int(total_page)\n",
    "\n",
    "for page_num_param in range(total_page):\n",
    "    url = findphd_url.format(page_num_param+1)\n",
    "    response = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    all_section = soup.find_all('div', class_='resultsRow')\n",
    "    \n",
    "    \n",
    "    for each_sec in all_section:\n",
    "        if each_sec.find('h3') is not None:\n",
    "            \n",
    "            title = each_sec.find('h3').get_text().strip() if each_sec.find('h3') is not None else ''\n",
    "            university = each_sec.find('a', class_='instLink').get_text().strip() if each_sec.find('a',class_='instLink') is not None else ''\n",
    "            professor = each_sec.find('a', class_='phd-result__key-info').get_text().strip().split(':')[1] if each_sec.find('a', class_='phd-result__key-info') is not None else ''\n",
    "            description = each_sec.find('div', class_='descFrag').get_text().split('Read')[0].strip() if each_sec.find('div', class_='descFrag') is not None else ''\n",
    "            string_with_deadline = each_sec.find('div', class_='phd-icon-area').get_text().strip().split('\\n')[0] if each_sec.find('div', class_='phd-icon-area') is not None else ''\n",
    "            link_to_project = 'https://www.findaphd.com' + each_sec.find('a', class_='h4').get('href') if each_sec.find('a', class_='h4') is not None else ''\n",
    "            f = open('computer_science_phd.csv', 'a', newline='', encoding=\"utf-8\")\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([title, university, professor, description, string_with_deadline, link_to_project])\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db9f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
